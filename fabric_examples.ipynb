{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532b8919",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa27f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package (run once)\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload powerbi_admin_api.py to your lakehouse Files folder first!\n",
    "# Then import it\n",
    "import sys\n",
    "sys.path.append('/lakehouse/default/Files')\n",
    "\n",
    "from powerbi_admin_api import PowerBIAdminAPI\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080b027",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b25218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Service Principal credentials\n",
    "TENANT_ID = \"your-tenant-id\"\n",
    "CLIENT_ID = \"your-app-client-id\"\n",
    "CLIENT_SECRET = \"your-app-client-secret\"\n",
    "\n",
    "# Output folder in lakehouse\n",
    "OUTPUT_FOLDER = \"/lakehouse/default/Files/pbi_scans\"\n",
    "\n",
    "# Initialize API client\n",
    "api = PowerBIAdminAPI(\n",
    "    tenant_id=TENANT_ID,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48731f",
   "metadata": {},
   "source": [
    "## Helper Function: Save to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_lakehouse(data, filename, output_folder=OUTPUT_FOLDER):\n",
    "    \"\"\"\n",
    "    Save data to lakehouse Files folder using mssparkutils.\n",
    "    Falls back to standard file write if mssparkutils unavailable.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Ensure output folder exists\n",
    "    full_path = f\"{output_folder}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        # Try using mssparkutils (Fabric native)\n",
    "        from notebookutils import mssparkutils\n",
    "        \n",
    "        json_content = json.dumps(data, indent=2)\n",
    "        mssparkutils.fs.put(full_path, json_content, overwrite=True)\n",
    "        print(f\"âœ… Saved to lakehouse: {full_path}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback to standard Python file operations\n",
    "        # Convert lakehouse path to local path\n",
    "        local_path = full_path.replace('/lakehouse/default/Files/', '')\n",
    "        \n",
    "        with open(local_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"âœ… Saved to local file: {local_path}\")\n",
    "    \n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe37473",
   "metadata": {},
   "source": [
    "## Example 1: Find Cloud Connections (Single Workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e552e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cloud_connections(workspace_id: str, output_folder=OUTPUT_FOLDER):\n",
    "    \"\"\"\n",
    "    Scan workspace and extract cloud connection information.\n",
    "    Saves results to lakehouse Files folder.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"SCANNING WORKSPACE FOR CLOUD CONNECTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Scan workspace with full metadata\n",
    "    scan_result = api.scan_workspace(\n",
    "        workspace_ids=workspace_id,\n",
    "        lineage=True,\n",
    "        datasource_details=True,\n",
    "        dataset_schema=True,\n",
    "        dataset_expressions=True\n",
    "    )\n",
    "    \n",
    "    # Extract datasource instances\n",
    "    datasources = scan_result.get('datasourceInstances', [])\n",
    "    print(f\"\\nâœ… Found {len(datasources)} datasource instance(s)\\n\")\n",
    "    \n",
    "    cloud_connections = []\n",
    "    for ds in datasources:\n",
    "        ds_type = ds.get('datasourceType', 'Unknown')\n",
    "        conn_details = ds.get('connectionDetails', {})\n",
    "        \n",
    "        print(f\"ðŸ“Š Datasource Type: {ds_type}\")\n",
    "        print(f\"   Connection Details: {json.dumps(conn_details, indent=3)}\")\n",
    "        print(f\"   Datasource ID: {ds.get('datasourceId')}\")\n",
    "        print(f\"   Gateway ID: {ds.get('gatewayId', 'None')}\")\n",
    "        print()\n",
    "        \n",
    "        cloud_connections.append({\n",
    "            'type': ds_type,\n",
    "            'connectionDetails': conn_details,\n",
    "            'datasourceId': ds.get('datasourceId'),\n",
    "            'gatewayId': ds.get('gatewayId')\n",
    "        })\n",
    "    \n",
    "    # Save full scan result to lakehouse\n",
    "    output_path = save_to_lakehouse(\n",
    "        scan_result, \n",
    "        f\"workspace_{workspace_id}_scan.json\",\n",
    "        output_folder\n",
    "    )\n",
    "    \n",
    "    # Save just the cloud connections summary\n",
    "    save_to_lakehouse(\n",
    "        cloud_connections,\n",
    "        f\"workspace_{workspace_id}_connections.json\",\n",
    "        output_folder\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    return cloud_connections\n",
    "\n",
    "# Execute: Replace with your workspace GUID\n",
    "# workspace_id = \"your-workspace-guid-here\"\n",
    "# connections = find_cloud_connections(workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4370d",
   "metadata": {},
   "source": [
    "## Example 2: Scan All Workspaces for Cloud Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_all_workspaces_for_connections(output_folder=OUTPUT_FOLDER, batch_size=100):\n",
    "    \"\"\"\n",
    "    Scan all workspaces in tenant and extract cloud connections.\n",
    "    Processes in batches and saves results to lakehouse.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"SCANNING ALL WORKSPACES FOR CLOUD CONNECTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Get all workspace IDs\n",
    "    print(\"\\nðŸ“‹ Retrieving workspace list...\")\n",
    "    workspaces = api.list_workspaces()\n",
    "    workspace_ids = [ws['id'] for ws in workspaces]\n",
    "    \n",
    "    print(f\"âœ… Found {len(workspace_ids)} workspaces\\n\")\n",
    "    \n",
    "    # Step 2: Scan in batches\n",
    "    all_connections = []\n",
    "    batch_results = []\n",
    "    \n",
    "    for i in range(0, len(workspace_ids), batch_size):\n",
    "        batch = workspace_ids[i:i+batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Scanning batch {batch_num}: {len(batch)} workspaces\")\n",
    "        \n",
    "        try:\n",
    "            # Scan batch\n",
    "            scan_result = api.scan_workspace(\n",
    "                workspace_ids=batch,\n",
    "                lineage=True,\n",
    "                datasource_details=True,\n",
    "                max_wait=900  # 15 minutes for large batches\n",
    "            )\n",
    "            \n",
    "            # Extract datasources from batch\n",
    "            datasources = scan_result.get('datasourceInstances', [])\n",
    "            print(f\"   âœ… Found {len(datasources)} datasource instances\")\n",
    "            \n",
    "            # Process each datasource\n",
    "            for ds in datasources:\n",
    "                connection_info = {\n",
    "                    'workspaceId': ds.get('workspaceId'),\n",
    "                    'datasourceType': ds.get('datasourceType'),\n",
    "                    'connectionDetails': ds.get('connectionDetails', {}),\n",
    "                    'datasourceId': ds.get('datasourceId'),\n",
    "                    'gatewayId': ds.get('gatewayId')\n",
    "                }\n",
    "                all_connections.append(connection_info)\n",
    "            \n",
    "            # Save batch result\n",
    "            save_to_lakehouse(\n",
    "                scan_result,\n",
    "                f\"scan_batch_{batch_num}.json\",\n",
    "                output_folder\n",
    "            )\n",
    "            \n",
    "            batch_results.append({\n",
    "                'batch': batch_num,\n",
    "                'workspaces_scanned': len(batch),\n",
    "                'datasources_found': len(datasources),\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "            # Rate limiting: wait between batches\n",
    "            if i + batch_size < len(workspace_ids):\n",
    "                print(\"   â³ Waiting 60 seconds before next batch...\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error scanning batch {batch_num}: {e}\")\n",
    "            batch_results.append({\n",
    "                'batch': batch_num,\n",
    "                'workspaces_scanned': len(batch),\n",
    "                'datasources_found': 0,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # Save consolidated results\n",
    "    print(\"\\nðŸ“Š Consolidating results...\")\n",
    "    \n",
    "    save_to_lakehouse(\n",
    "        all_connections,\n",
    "        \"all_cloud_connections.json\",\n",
    "        output_folder\n",
    "    )\n",
    "    \n",
    "    save_to_lakehouse(\n",
    "        batch_results,\n",
    "        \"scan_summary.json\",\n",
    "        output_folder\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCAN COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total workspaces scanned: {len(workspace_ids)}\")\n",
    "    print(f\"Total cloud connections found: {len(all_connections)}\")\n",
    "    print(f\"Successful batches: {sum(1 for b in batch_results if b['status'] == 'success')}\")\n",
    "    print(f\"Failed batches: {sum(1 for b in batch_results if b['status'] == 'failed')}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return all_connections\n",
    "\n",
    "# Execute full tenant scan\n",
    "# all_connections = scan_all_workspaces_for_connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a516e0",
   "metadata": {},
   "source": [
    "## Example 3: Analyze Cloud Connection Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_connection_types(connections_file=\"all_cloud_connections.json\"):\n",
    "    \"\"\"\n",
    "    Analyze cloud connections by type from saved results.\n",
    "    \"\"\"\n",
    "    from notebookutils import mssparkutils\n",
    "    \n",
    "    # Load connections from lakehouse\n",
    "    connections_path = f\"{OUTPUT_FOLDER}/{connections_file}\"\n",
    "    content = mssparkutils.fs.head(connections_path, 10000000)  # Read up to 10MB\n",
    "    connections = json.loads(content)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CLOUD CONNECTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count by type\n",
    "    type_counts = {}\n",
    "    for conn in connections:\n",
    "        ds_type = conn.get('datasourceType', 'Unknown')\n",
    "        type_counts[ds_type] = type_counts.get(ds_type, 0) + 1\n",
    "    \n",
    "    # Display sorted by count\n",
    "    print(f\"\\nTotal Connections: {len(connections)}\\n\")\n",
    "    print(\"Connection Types:\")\n",
    "    for ds_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {count:4d} - {ds_type}\")\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis = {\n",
    "        'total_connections': len(connections),\n",
    "        'types': type_counts,\n",
    "        'analysis_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    save_to_lakehouse(analysis, \"connection_analysis.json\", OUTPUT_FOLDER)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    return analysis\n",
    "\n",
    "# Execute analysis\n",
    "# analysis = analyze_connection_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a26661",
   "metadata": {},
   "source": [
    "## Example 4: Quick Test - Single Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with a single workspace\n",
    "workspace_id = \"your-workspace-guid-here\"\n",
    "\n",
    "# Find cloud connections and save to lakehouse\n",
    "connections = find_cloud_connections(workspace_id)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nFound {len(connections)} cloud connections:\")\n",
    "for conn in connections:\n",
    "    print(f\"  - {conn['type']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
